{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9285dc",
   "metadata": {},
   "source": [
    "# 그룹별 Tension 예측을 위한 데이터 전처리 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24dfec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bee7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def group_sec(data , freq):\n",
    "    \n",
    "    data['datetime'] =  pd.to_datetime(data['timestamp'], unit='s')\n",
    "    \n",
    "                \n",
    "    #data['datetime'] = pd.to_timedelta(data['timestamp'], unit='s')\n",
    "    df = data.set_index('datetime')\n",
    "    \n",
    "    ten_sec_df = df.groupby(pd.Grouper(freq=freq)).agg('max')\n",
    "    ten_sec_df = ten_sec_df.interpolate()\n",
    "\n",
    "    ten_sec_df = ten_sec_df.reset_index()\n",
    "    return ten_sec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4856a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "108\n",
      "118\n",
      "105\n",
      "113\n",
      "116\n",
      "110\n",
      "104\n",
      "115\n",
      "111\n",
      "101\n",
      "114\n",
      "103\n",
      "106\n",
      "119\n",
      "102\n",
      "107\n",
      "117\n",
      "120\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "pre_pre_dir = '/workspace/data/dataset_2019/'\n",
    "for d in os.listdir(pre_pre_dir):\n",
    "\n",
    "    pre_dir = pre_pre_dir + d +\"/\"\n",
    "    lst_timestamp = os.listdir(pre_dir)\n",
    "    print(d)\n",
    "\n",
    "    for ts in lst_timestamp:\n",
    "        time_pre_dir = pre_dir + ts + '/'\n",
    "        for ma in os.listdir(time_pre_dir):\n",
    "            if ma[-4] == '.':\n",
    "                continue\n",
    "            t = time_pre_dir+ma+'/'\n",
    "            df = pd.DataFrame()\n",
    "            for detail_timestamp in os.listdir(t):\n",
    "                file_dir = t + detail_timestamp\n",
    "\n",
    "                tmp_data = pd.read_csv(file_dir)\n",
    "                tmp_data['timestamp'] += int(detail_timestamp.replace('.csv',''))\n",
    "\n",
    "                tmp_data = group_sec(tmp_data,'5s')\n",
    "                tmp_data.rename(columns = lambda x: ma +\"__\"+ x, inplace = True)\n",
    "\n",
    "                #print(tmp_data.shape)\n",
    "                df = pd.concat([df,tmp_data],ignore_index=True)\n",
    "            df.to_csv(time_pre_dir+ma+'.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713014b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data/dataset_2019/109/\n",
      "/workspace/data/dataset_2019/108/\n",
      "/workspace/data/dataset_2019/118/\n",
      "/workspace/data/dataset_2019/105/\n",
      "/workspace/data/dataset_2019/113/\n",
      "/workspace/data/dataset_2019/116/\n",
      "/workspace/data/dataset_2019/110/\n",
      "/workspace/data/dataset_2019/104/\n",
      "/workspace/data/dataset_2019/115/\n",
      "/workspace/data/dataset_2019/111/\n",
      "/workspace/data/dataset_2019/101/\n",
      "/workspace/data/dataset_2019/114/\n",
      "/workspace/data/dataset_2019/103/\n",
      "/workspace/data/dataset_2019/106/\n",
      "/workspace/data/dataset_2019/119/\n",
      "/workspace/data/dataset_2019/102/\n",
      "/workspace/data/dataset_2019/107/\n",
      "/workspace/data/dataset_2019/117/\n",
      "/workspace/data/dataset_2019/120/\n",
      "/workspace/data/dataset_2019/112/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "ppre_dir = '/workspace/data/dataset_2019/'\n",
    "\n",
    "for filename in os.listdir(ppre_dir):\n",
    "    \n",
    "    _pre_dir = ppre_dir + filename+'/'\n",
    "    df = pd.DataFrame()\n",
    "    print(_pre_dir)\n",
    "    for d in os.listdir(_pre_dir):\n",
    "        cnt = 0 \n",
    "        pre_dir = _pre_dir + d + '/'\n",
    "        check = False\n",
    "        for file in os.listdir(pre_dir):\n",
    "            if  ('.' not in file )or ('label' in file ):\n",
    "                continue\n",
    "            #print(file)\n",
    "            if cnt == 0 :\n",
    "                cnt +=1 \n",
    "                t_df = pd.read_csv(pre_dir + file,index_col=0)\n",
    "                if t_df.shape[0] <= 0 :\n",
    "                    break\n",
    "                t_df.rename(columns={ file.replace('.csv','')+'__datetime':'datetime'}, inplace = True)\n",
    "\n",
    "                t_df['datetime'] = pd.to_datetime(t_df['datetime']).astype(str)\n",
    "                t_df =t_df.drop(columns = [file.replace('.csv','')+'__timestamp'])\n",
    "                #e4Acc__timestamp\n",
    "\n",
    "                continue\n",
    "\n",
    "            tmp_df = pd.read_csv(pre_dir + file,index_col=0)\n",
    "            if tmp_df.shape[0] <= 0 :\n",
    "                break        \n",
    "            word = file.replace('.csv','')+'__timestamp'\n",
    "\n",
    "\n",
    "            tmp_df =tmp_df.drop(columns = [file.replace('.csv','')+'__timestamp'])\n",
    "\n",
    "            tmp_df.rename(columns={ file.replace('.csv','')+'__datetime':'datetime'}, inplace = True)\n",
    "            tmp_df['datetime'] = pd.to_datetime(tmp_df['datetime']).astype(str)\n",
    "            t_df = pd.merge(t_df, tmp_df, on=['datetime'], how = 'inner')\n",
    "\n",
    "\n",
    "        df = pd.concat([df,t_df])\n",
    "    df = df.set_index(['datetime'])\n",
    "    df = df.sort_index()\n",
    "    #ppre_dir ='/workspace/data/user01-06/user02/'\n",
    "\n",
    "    lst = os.listdir(_pre_dir)\n",
    "    concat_label = pd.DataFrame()\n",
    "    for f in lst:\n",
    "        \n",
    "        file_path = f'{_pre_dir}{f}/{f}_label.csv'\n",
    "        #print(file_path)\n",
    "        label = pd.read_csv(file_path)\n",
    "        if label.shape[0] <= 0 :\n",
    "            continue\n",
    "        label['datetime'] =  pd.to_datetime(label['ts'], unit='s')\n",
    "\n",
    "        label = label.set_index(['datetime'])\n",
    "        #print(label.index.duplicated())\n",
    "        #duplicated_values = label[label.index.duplicated()]\n",
    "        #print(duplicated_values)\n",
    "        label = label[~label.index.duplicated()]\n",
    "        label_5s = label.resample('5s').ffill()\n",
    "\n",
    "        concat_label = pd.concat([concat_label,label_5s])\n",
    "    concat_label = concat_label.sort_index()\n",
    "    concat_label = concat_label.drop(columns = ['ts'])\n",
    "    df = df.reset_index()\n",
    "    concat_label = concat_label.reset_index()\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    concat_label['datetime'] = pd.to_datetime(concat_label['datetime'])\n",
    "    merged_df = pd.merge(df, concat_label, on='datetime', how='inner')\n",
    "    merged_df.fillna(0)\n",
    "    merged_df = merged_df.set_index('datetime')\n",
    "    merged_df.to_csv(f'processed_data/tmp_merged_{filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589fdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_data/\n",
      "processed_data/tmp_merged_110.csv\n",
      "processed_data/tmp_merged_109.csv\n",
      "processed_data/tmp_merged_101.csv\n",
      "processed_data/tmp_merged_104.csv\n",
      "processed_data/tmp_merged_102.csv\n",
      "processed_data/tmp_merged_106.csv\n",
      "processed_data/tmp_merged_118.csv\n",
      "processed_data/tmp_merged_117.csv\n",
      "processed_data/tmp_merged_116.csv\n",
      "processed_data/tmp_merged_103.csv\n",
      "processed_data/tmp_merged_113.csv\n",
      "processed_data/tmp_merged_107.csv\n",
      "processed_data/tmp_merged_119.csv\n",
      "processed_data/tmp_merged_114.csv\n",
      "processed_data/tmp_merged_108.csv\n",
      "processed_data/tmp_merged_111.csv\n",
      "processed_data/tmp_merged_105.csv\n",
      "processed_data/tmp_merged_120.csv\n",
      "processed_data/tmp_merged_112.csv\n",
      "processed_data/tmp_merged_115.csv\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf tmp_processed.tar processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216d4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
